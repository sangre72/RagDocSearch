"""HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬"""

import os
import shutil
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field
from datetime import datetime


@dataclass
class DownloadStatus:
    """ë‹¤ìš´ë¡œë“œ ìƒíƒœ"""
    model_name: str
    status: str  # "pending", "downloading", "completed", "failed"
    progress: float = 0.0
    error: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


@dataclass
class CachedModel:
    """ìºì‹œëœ ëª¨ë¸ ì •ë³´"""
    name: str
    path: str
    size_bytes: int
    downloaded_at: datetime


class ModelDownloader:
    """HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬"""

    # ì¶”ì²œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡
    RECOMMENDED_EMBEDDING_MODELS = {
        "sentence-transformers/all-MiniLM-L6-v2": {
            "dimension": 384,
            "description": "âš¡ ë¹ ë¥¸ ì†ë„, ê°€ë²¼ìš´ ìš©ëŸ‰ (ì˜ì–´ ìµœì í™”, ë‹¤êµ­ì–´ ê¸°ë³¸ ì§€ì›)",
            "size_mb": 90,
        },
        "sentence-transformers/all-mpnet-base-v2": {
            "dimension": 768,
            "description": "ğŸ† ì˜ì–´ ë¬¸ì„œ ìµœê³  ì„±ëŠ¥ (ì˜ì–´ ì „ìš©)",
            "size_mb": 420,
        },
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2": {
            "dimension": 384,
            "description": "ğŸŒ 50ê°œ ì–¸ì–´ ì§€ì› (í•œêµ­ì–´ í¬í•¨, ê· í˜•ì¡íŒ ì„±ëŠ¥)",
            "size_mb": 470,
        },
        "BAAI/bge-m3": {
            "dimension": 1024,
            "description": "ğŸš€ ìµœì‹  SOTA ëª¨ë¸ (í•œêµ­ì–´ ìš°ìˆ˜, Dense+Sparse ê²€ìƒ‰)",
            "size_mb": 2200,
        },
        "intfloat/multilingual-e5-large": {
            "dimension": 1024,
            "description": "ğŸ’ª ëŒ€ìš©ëŸ‰ ë‹¤êµ­ì–´ ëª¨ë¸ (í•œêµ­ì–´ ìš°ìˆ˜, ë†’ì€ ì •í™•ë„)",
            "size_mb": 2200,
        },
    }

    # ì¶”ì²œ LLM ëª¨ë¸ ëª©ë¡ (HuggingFace Transformers ìš©)
    RECOMMENDED_LLM_MODELS = {
        "microsoft/phi-2": {
            "description": "âš¡ 2.7B ê²½ëŸ‰ ê³ ì„±ëŠ¥ (ì˜ì–´ ìµœì í™”, ì½”ë“œ ìƒì„± ìš°ìˆ˜)",
            "size_gb": 5.4,
        },
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0": {
            "description": "ğŸª¶ 1.1B ì´ˆê²½ëŸ‰ ì±„íŒ… (ë¹ ë¥¸ ì‘ë‹µ, ì €ì‚¬ì–‘ PC)",
            "size_gb": 2.2,
        },
        "Qwen/Qwen2-1.5B-Instruct": {
            "description": "ğŸ‡°ğŸ‡· 1.5B í•œêµ­ì–´ ì§€ì› (í•œì¤‘ì˜ ë‹¤êµ­ì–´, ê· í˜•ì¡íŒ ì„±ëŠ¥)",
            "size_gb": 3.0,
        },
    }

    _download_status: dict[str, DownloadStatus] = field(default_factory=dict)

    def __init__(self, cache_dir: str = "./models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._download_status: dict[str, DownloadStatus] = {}

    def _get_model_path(self, model_name: str) -> Path:
        """ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë°˜í™˜"""
        safe_name = model_name.replace("/", "--")
        return self.cache_dir / safe_name

    def is_model_cached(self, model_name: str) -> bool:
        """ëª¨ë¸ì´ ìºì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        model_path = self._get_model_path(model_name)
        return model_path.exists() and any(model_path.iterdir())

    def get_cached_models(self) -> list[CachedModel]:
        """ìºì‹œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        cached = []
        if not self.cache_dir.exists():
            return cached

        for model_dir in self.cache_dir.iterdir():
            if model_dir.is_dir():
                name = model_dir.name.replace("--", "/")
                size = sum(
                    f.stat().st_size
                    for f in model_dir.rglob("*")
                    if f.is_file()
                )
                mtime = datetime.fromtimestamp(model_dir.stat().st_mtime)
                cached.append(CachedModel(
                    name=name,
                    path=str(model_dir),
                    size_bytes=size,
                    downloaded_at=mtime,
                ))

        return cached

    def get_download_status(self, model_name: str) -> Optional[DownloadStatus]:
        """ë‹¤ìš´ë¡œë“œ ìƒíƒœ ì¡°íšŒ"""
        return self._download_status.get(model_name)

    def delete_model(self, model_name: str) -> bool:
        """ìºì‹œëœ ëª¨ë¸ ì‚­ì œ"""
        model_path = self._get_model_path(model_name)
        if model_path.exists():
            shutil.rmtree(model_path)
            return True
        return False

    def get_available_embedding_models(self) -> dict:
        """ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡"""
        return self.RECOMMENDED_EMBEDDING_MODELS

    def get_available_llm_models(self) -> dict:
        """ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ LLM ëª¨ë¸ ëª©ë¡"""
        return self.RECOMMENDED_LLM_MODELS

    async def download_embedding_model(
        self,
        model_name: str,
        progress_callback: Optional[callable] = None
    ) -> str:
        """ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (sentence-transformers)"""
        from sentence_transformers import SentenceTransformer

        self._download_status[model_name] = DownloadStatus(
            model_name=model_name,
            status="downloading",
            started_at=datetime.now(),
        )

        try:
            model_path = self._get_model_path(model_name)

            # sentence-transformersê°€ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
            model = SentenceTransformer(
                model_name,
                cache_folder=str(self.cache_dir),
            )

            self._download_status[model_name] = DownloadStatus(
                model_name=model_name,
                status="completed",
                progress=100.0,
                completed_at=datetime.now(),
            )

            return str(model_path)

        except Exception as e:
            self._download_status[model_name] = DownloadStatus(
                model_name=model_name,
                status="failed",
                error=str(e),
            )
            raise

    async def download_llm_model(
        self,
        model_name: str,
        progress_callback: Optional[callable] = None
    ) -> str:
        """LLM ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (transformers)"""
        from huggingface_hub import snapshot_download

        self._download_status[model_name] = DownloadStatus(
            model_name=model_name,
            status="downloading",
            started_at=datetime.now(),
        )

        try:
            model_path = self._get_model_path(model_name)

            # huggingface_hubë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            snapshot_download(
                repo_id=model_name,
                local_dir=str(model_path),
                local_dir_use_symlinks=False,
            )

            self._download_status[model_name] = DownloadStatus(
                model_name=model_name,
                status="completed",
                progress=100.0,
                completed_at=datetime.now(),
            )

            return str(model_path)

        except Exception as e:
            self._download_status[model_name] = DownloadStatus(
                model_name=model_name,
                status="failed",
                error=str(e),
            )
            raise
